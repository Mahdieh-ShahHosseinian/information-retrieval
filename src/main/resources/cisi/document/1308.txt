  A new scientific theory has been born during the last few years, the theoryof information.  It immediately attracted a great deal of interest and hasexpanded very rapidly.  This new theory was initially the result of a verypractical and utilitarian discussion of certain basic problems:  How is itpossible to define the quantity of information contained in a message or telegram to be transmitted?  How does one measure the amount of informationcommunicated by a system of telegraphic signals?  How does one compare thesetwo qualities and discuss the efficiency for coding devices?  All of theseproblems, and many similar ones, are of concern to the telecommunicationengineer and can now be discussed quantitatively.  From these discussions there emerged a new theory of both mathematicaland practical character.  This theory is based on probability considerations.Once stated in a precise way, it can be used for many fundamental scientificdiscussions.  It enables one to solve the problem of Maxwell's demon and toshow a very direct connection between information and entropy.  The thermodynamic entropy measures the lack of information about a certainphysical system.  Whenever an experiment is performed in the laboratory,it is paid for by an increase of entropy, and a generalized Carnot Principlestates that the price paid in increase of entropy must always be larger thanthe amount of information gained.  Information corresponds to negativeentropy, a quantity for which the author coined the word negentropy.  Thegeneralized Carnot Principle may also be called the negentropy principle ofinformation.  This principle imposes a new limitation on physical experimentsand is independent of the well-known uncertainty relations of quantum mechanics.
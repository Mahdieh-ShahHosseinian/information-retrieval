   An empirical investigation of the role of documents in relevance judgementsis reported.. Abstracts previously judged relevant, partially relevant, and nonrelevant to each of 61 questions were compared to see whether textual differences could be found which might reasonably account for the rating differences.. The results of this comparison were fairly clear-cut characterizations in each case of relevant and partially relevant abstracts.. These characterizations were found to be expressible largely as meaningfulco-occurrences of terms closely related to the question.. It is suggested that the textual bases of user choices may be more understandable than has been supposed..
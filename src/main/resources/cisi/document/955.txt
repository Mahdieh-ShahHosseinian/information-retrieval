   The relevance assessments belonging to the Cranfield II document/querycollection are shown to be faulty, in the sense that "many" relevant documentswere not so identified by the Cranfield judges.. The implications of theseomissions for the evaluation of information retrieval experiments based on theCranfield collection are examined in detail..It is shown that numerical measuresof retrieval effectiveness may be greatly altered bu consideration of the "missing" relevant documents and that a ranking  of retrieval methods according to order of performance may vary as well..